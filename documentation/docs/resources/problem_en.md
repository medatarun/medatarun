---
sidebar_position: 2
---

# The Missing Operational Data Model

:::note
    This text had been translated by AI from French [üá´üá∑Le mod√®le de donn√©es op√©rationnel manquant](./problem_fr.md)
:::

For more than 30 years, I‚Äôve been running into the same problem over and over again.

Every time I join a software project, I ask the same simple question: what data are we actually managing? I ask
developers, project managers, business people, and every time the answer is the same: nobody can really answer.

- Business teams show me screens, which are only partial, transformed interpretations of the data. 
- Developers tell me to look at database schemas and random files scattered through the codebase, buried in business logic and persistence
layers, in other words: the implementation. 
- Project managers say ‚Äúwith a bit of luck it‚Äôs up to date on Confluence‚Äù, which is usually a bad sign. 
- Sometimes someone pulls out a Word document written ten years ago that didn‚Äôt survive the
evolution of the software.

Nobody can give a global view: relationships, business meaning, invariants, operations. I end up doing document
archaeology, endless interviews, cross-checking information, digging through piles of code, SQL, and live databases that
are rarely accessible. And once it‚Äôs all in my head, what then? Do it all again on the next project?

## So we just need to document it? Fail.

Every attempt at documenting data has failed. It‚Äôs always heavy, slow work, disconnected from software that keeps
evolving. Nobody wants to maintain it, and little by little the documentation dies. I tried automation too, in both
directions: conceptual business models pushed toward code with layers of transformers and manual work, code
introspection with annotations, SQL exported to wikis. Nothing worked.

Why? Because in practice, documentation brings no operational value.

Another point: we‚Äôve always compensated by telling ourselves that this knowledge lived in people‚Äôs heads. Collective
human knowledge was ‚Äúgood enough‚Äù.

Most of the time, it worked. Until it didn‚Äôt.

Some examples from experience: 
- the only person who understood inheritance and succession logic left, making the project
unmaintainable and impossible to update for regulatory reasons, so it had to be abandoned. 
- The people who denormalized the data were gone, nobody could explain why, and by the time we understood, it was too late: customers were blocked by
performance issues. 
- A client urgently asks for a GDPR-style data report before buying: nights spent digging everywhere. 
- Governance blocks changes in architecture review boards because there is no visibility.

There are plenty more stories like that.

## Rise of Governance

The context has changed.

Data governance is now front and center, driven by regulations piling up: GDPR, ISO 27001, and others. Today we need to
know exactly what data we have, what we process, where it lives, which regulations apply, whether it‚Äôs sensitive.
Listing and cataloging all this data is necessary, tedious, unproductive‚Ä¶ and frankly painful.

In most companies I‚Äôve worked with, this is handled through occasional audits that end up as massive Excel spreadsheets.
They‚Äôre never maintained, never updated. The process comes with endless meetings to analyze, collect, and understand the
data. Six months later, the work is already obsolete. Even when data management is included in governance processes,
steering committees, architecture boards, it still fails.

Once again, there is no operational reality behind it. It‚Äôs about compliance, not value. A constraint, not an asset. But
it has to be done.

## Here Comes a New Challenger

And then a new player arrived: AI, along with agents and assistants. At first it‚Äôs impressive. Give them database
schemas and some code, and they manage reasonably well. It even looks admirable with modern, well-designed databases.

Now give them your slightly legacy projects, or the ones built in a hurry. You know the ones: columns limited to six
characters, everything stored as VARCHAR, even dates. Or just CSV headers, for a laugh.

Very quickly, you hit a wall: the AI **guesses** the meaning of data, and often guesses wrong.

In the end, it‚Äôs no worse than humans. Without explanation, misunderstanding is the same, it just happens faster.

There‚Äôs no real dialogue to explain things gradually as the AI explores. It has no access to the shared ‚Äúwe all know‚Äù
understanding, nor to what was in the project manager‚Äôs head, or the business owner‚Äôs, or the developer‚Äôs, especially
when they‚Äôve been gone for years.

At some point, documentation is no longer a ‚Äúnice to have‚Äù. It becomes **mandatory**.

# Same Data, Different Views

So how do you manage a data documentation system that works for business teams, developers, IT, data analysts, and AI
agents at the same time?

That problem is still unsolved.

Each group has different needs and perspectives. Business users don‚Äôt operate in a way that lets them talk data or
technical concepts. Technical teams think in code, tables, and types, which means nothing to business and often pushes
them away. Governance needs to know what exists and be able to generate reports to decide. Project managers need to know
what‚Äôs up to date, and be warned when data structures evolve in chaotic ways. Architects need visibility without
spending nights chasing projects to detect new sensitive data or changes in usage.

Everyone works on the same thing, but not in the same way, and not with the same view.

That‚Äôs also why every change meets resistance and political deadlock forms so quickly.

The artifacts we‚Äôve used so far, Excel files, Word documents, Confluence pages, database schemas, are not connected.
Business people won‚Äôt read database schemas. Excel files are obsolete the moment they‚Äôre produced and only interest
governance.

## A Static Problem

Another issue: these documentation artifacts are static. They don‚Äôt live, they don‚Äôt interact with anything. They
describe after the fact what the system already does, or worse, what people think it does. That‚Äôs why they die. It‚Äôs not
negligence or bad will, it‚Äôs simply useless in real workflows.

By contrast, database schemas, extraction code, ETLs are maintained because they are operational. If they break, the
whole system breaks.

## A Possible Path

The blind spot in all this is that we‚Äôve never treated the conceptual data model as a living system: queryable,
modifiable, versioned, integrated into development processes, with proper user interfaces, search, reporting, and
everything needed to work with it operationally.

With AI, this becomes urgent. We need a system that can be queried through APIs or MCP, so AI can understand data: its
meaning, invariants, usage, what‚Äôs inside. That‚Äôs how we generate real help for BI users, precise user stories,
assistance for developers and agents to code and document, and allow governance to query what it owns in human terms.

Conceptual data models must become explicit, shared, versioned objects, enriched and directly usable by humans across
roles, by AI, by CI/CD automation, by the whole technical chain.

This canonical model now has intrinsic value: it is consulted to understand and enrich, used to decide, exploited to
generate, validate, analyze, and precise enough for AI to consume without guessing. It already **knows**.

Once such a model exists and is actually useful, the dynamic reverses. Maintaining it is no longer ‚Äúextra‚Äù. It becomes a
condition for the system to function.

The question stops being ‚Äúhow do we document data?‚Äù and becomes ‚Äúhow do we keep a shared, operational data model alive,
for all actors in the system, human and technical alike?‚Äù

## Your Feedback

That‚Äôs where this project comes from. Not to build yet another governance tool, there are plenty of those, but because I
never found a satisfying answer to this problem.

So I started this: [Medatarun](https://github.com/medatarun/medatarun).

I‚Äôm not trying to convince anyone. What matters to me now is understanding how this works where you are. Do you face the
same problems? Does this gap in real data understanding resonate with you? Does this description match your experience,
or do you see the problem differently?

I‚Äôm mainly interested in feedback: experiences, disagreements, things I‚Äôve missed.

And if you‚Äôre facing this kind of problem and don‚Äôt know where to start, that‚Äôs exactly how this project began. Come and
talk about it.